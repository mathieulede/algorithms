{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Even more text analysis with scikit-learn\n",
    "\n",
    "We've spent the past week counting words, *and we're just going to keep right on doing it.*\n",
    "\n",
    "The technical term for this is **bag of words** analysis, because it doesn't care about what order the words are in. It's like you just took all of the words in a speech or a book or whatever and just dumped them into a bag. A bag of words.\n",
    "\n",
    "It seems like it would be terrible but it really gets the job done.\n",
    "\n",
    "# Even more dumb sentences\n",
    "\n",
    "We can't let go of fish, bugs, and Penny. But this time we also have some cats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"Penny bought bright blue fishes.\",\n",
    "    \"Penny bought bright blue and orange fish.\",\n",
    "    \"The cat ate a fish at the store.\",\n",
    "    \"Penny went to the store. Penny ate a bug. Penny saw a fish.\",\n",
    "    \"It meowed once at the bug, it is still meowing at the bug and the fish\",\n",
    "    \"The cat is at the fish store. The cat is orange. The cat is meowing at the fish.\",\n",
    "    \"Penny is a fish\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise A : Put these sentences into TWO sensible groups\n",
    "\n",
    "Not with programming, just with your brain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"The cat ate a fish at the store.\",\n",
    "    \"It meowed once at the bug, it is still meowing at the bug and the fish\",\n",
    "    \"The cat is at the fish store. The cat is orange. The cat is meowing at the fish.\",\n",
    "]\n",
    "\n",
    "texts = [\n",
    "    \"Penny bought bright blue fishes.\",\n",
    "    \"Penny bought bright blue and orange fish.\",\n",
    "    \"Penny went to the store. Penny ate a bug. Penny saw a fish.\",\n",
    "    \"Penny is a fish\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise B: Put these sentences into THREE groups based on their content\n",
    "\n",
    "Again, not with programming, just with your brain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"Penny bought bright blue fishes.\",\n",
    "    \"Penny bought bright blue and orange fish.\",\n",
    "    \"The cat ate a fish at the store.\",\n",
    "    \"Penny went to the store. Penny ate a bug. Penny saw a fish.\",\n",
    "    \"It meowed once at the bug, it is still meowing at the bug and the fish\",\n",
    "    \"The cat is at the fish store. The cat is orange. The cat is meowing at the fish.\",\n",
    "    \"Penny is a fish\"\n",
    "]\n",
    "\n",
    "texts = [\n",
    "    \"Penny bought bright blue fishes.\",\n",
    "    \"Penny bought bright blue and orange fish.\",\n",
    "    \"The cat ate a fish at the store.\",\n",
    "    \"Penny went to the store. Penny ate a bug. Penny saw a fish.\",\n",
    "    \"It meowed once at the bug, it is still meowing at the bug and the fish\",\n",
    "    \"The cat is at the fish store. The cat is orange. The cat is meowing at the fish.\",\n",
    "    \"Penny is a fish\"\n",
    "]\n",
    "\n",
    "texts = [\n",
    "    \"Penny bought bright blue fishes.\",\n",
    "    \"Penny bought bright blue and orange fish.\",\n",
    "    \"The cat ate a fish at the store.\",\n",
    "    \"Penny went to the store. Penny ate a bug. Penny saw a fish.\",\n",
    "    \"It meowed once at the bug, it is still meowing at the bug and the fish\",\n",
    "    \"The cat is at the fish store. The cat is orange. The cat is meowing at the fish.\",\n",
    "    \"Penny is a fish\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, on to the computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already know how to **vectorize**, how to convert sentences into numeric representations. We use a **vectorizer!** There are two options we've learned about, the `CountVectorizer` and the `TfidfVectorizer`.\n",
    "\n",
    "* `CountVectorizer`: count the words\n",
    "* `TfidfVectorizer`: percentage of the words in a sentence (kind of)\n",
    "\n",
    "### CountVectorizer\n",
    "\n",
    "Just normal counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer()\n",
    "matrix = vec.fit_transform(texts)\n",
    "pd.DataFrame(matrix.toarray(), columns=vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer\n",
    "\n",
    "So far we've used `TfIdfVectorizer` to compare sentences of different length (your name in a tweet vs. your name in a book)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vec = TfidfVectorizer(use_idf=False, norm='l1')\n",
    "matrix = vec.fit_transform(texts)\n",
    "pd.DataFrame(matrix.toarray(), columns=vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "That all seems fine, but we need to combine `meow` and `meowing` and whatever else, yeah? We'll use TextBlob for that, and give our vectorizer a custom tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def textblob_tokenizer(str_input):\n",
    "    blob = TextBlob(str_input.lower())\n",
    "    tokens = blob.words\n",
    "    words = [token.stem() for token in tokens]\n",
    "    return words\n",
    "\n",
    "vec = CountVectorizer(tokenizer=textblob_tokenizer)\n",
    "matrix = vec.fit_transform(texts)\n",
    "pd.DataFrame(matrix.toarray(), columns=vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ...oh, and stopwords\n",
    "\n",
    "And let's get rid of stopwords, too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(tokenizer=textblob_tokenizer, stop_words='english')\n",
    "matrix = vec.fit_transform(texts)\n",
    "pd.DataFrame(matrix.toarray(), columns=vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section One: Term Frequency (TF)\n",
    "\n",
    "We've talked about **term frequency** before, it's just the percentage of times the words are used in a sentence. Let's refresh what our sentences are, then use a `TfidfVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"Penny bought bright blue fishes.\",\n",
    "    \"Penny bought bright blue and orange fish.\",\n",
    "    \"The cat ate a fish at the store.\",\n",
    "    \"Penny went to the store. Penny ate a bug. Penny saw a fish.\",\n",
    "    \"It meowed once at the bug, it is still meowing at the bug and the fish\",\n",
    "    \"The cat is at the fish store. The cat is orange. The cat is meowing at the fish.\",\n",
    "    \"Penny is a fish\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We have to use these other parameters because I SAID SO\n",
    "vec = TfidfVectorizer(tokenizer=textblob_tokenizer,\n",
    "                      stop_words='english',\n",
    "                      norm='l1', # ELL - ONE\n",
    "                      use_idf=False)\n",
    "matrix = vec.fit_transform(texts)\n",
    "df = pd.DataFrame(matrix.toarray(), columns=vec.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which sentence is the most about fish?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.sort_values(by='fish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about fish AND meowing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[['fish', 'meow']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.meow + df.fish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    'fish': df.fish,\n",
    "    'meow': df.meow,\n",
    "    'meow + fish': df.meow + df.fish\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like index `4` and `6` are tied, but `meow` doesn't even show up in six! That's no good, or at least it *seems* silly.\n",
    "\n",
    "It seems like since `fish` shows up again and again it should be weighted a little less - not like it’s a stopword, but just... it’s kind of cliche to have it show up in the text, so we want to make it less important.\n",
    "\n",
    "So maybe, you know **popular words should be less important.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section Two: Inverse Document Frequency (IDF)\n",
    "\n",
    "The concept that words that are more popular across all of the documents should be less important is **inverse document frequency!** We're going to try it again, this time changing `use_idf=False` to `use_idf=True`. The vectorizer actually uses inverse document frequency by default, but this will help us remember what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We have to use these other parameters because I SAID SO\n",
    "vec = TfidfVectorizer(tokenizer=textblob_tokenizer,\n",
    "                      stop_words='english',\n",
    "                      norm='l1',\n",
    "                      use_idf=True)\n",
    "matrix = vec.fit_transform(texts)\n",
    "idf_df = pd.DataFrame(matrix.toarray(), columns=vec.get_feature_names())\n",
    "idf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OLD dataframe\n",
    "pd.DataFrame({\n",
    "    'fish': df.fish,\n",
    "    'meow': df.meow,\n",
    "    'meow + fish': df.meow + df.fish\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NEW dataframe\n",
    "pd.DataFrame({\n",
    "    'fish': idf_df.fish,\n",
    "    'meow': idf_df.meow,\n",
    "    'meow + fish': idf_df.meow + idf_df.fish\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so things changed a little, but **I'm honestly not that impressed.**'\n",
    "\n",
    "You know how we've been setting `norm=l1` all of the time. By default it's actually uses an `l2`(Euclidean) norm, which works a lot better, pulling apart the differences between sentences. Why? I don't know. What does it mean? I don't know. How does it work? I don't know. But let's get rid of that \"ELL ONE\" in order to work with the defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We have to *get rid of* norm='l1' because I SAID SO\n",
    "vec = TfidfVectorizer(tokenizer=textblob_tokenizer,\n",
    "                      stop_words='english',\n",
    "                      use_idf=True)\n",
    "matrix = vec.fit_transform(texts)\n",
    "idf_df = pd.DataFrame(matrix.toarray(), columns=vec.get_feature_names())\n",
    "idf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OLD dataframe\n",
    "pd.DataFrame({\n",
    "    'fish': df.fish,\n",
    "    'meow': df.meow,\n",
    "    'meow + fish': df.meow + df.fish\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NEW dataframe\n",
    "pd.DataFrame({\n",
    "    'fish': idf_df.fish,\n",
    "    'meow': idf_df.meow,\n",
    "    'meow + fish': idf_df.meow + idf_df.fish\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now *that's* a lot better. Look at index 4! It's amazing! Sure, we have a **fish** but that **meow** is just powering beyond anything known to humankind!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section Three: Document Similarity\n",
    "\n",
    "## Who cares? Why do we need to know this?\n",
    "\n",
    "When someone dumps 100,000 documents on your desk in response to FOIA, you’ll start to care! One of the reasons understanding TF-IDF is important is because of **document similarity**. By knowing what documents are similar you’re able to find related documents and automatically group documents into clusters.\n",
    "\n",
    "For example! Let’s cluster these documents using **K-Means clustering** (check out this [gif](http://practicalcryptography.com/media/miscellaneous/files/k_mean_send.gif)). K means basically plots all of the numbers on a graph and grabs the ones that group together. It doesn't make sense right now, but we'll do a simpler example in a second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We have to use these other parameters because I SAID SO\n",
    "vec = TfidfVectorizer(tokenizer=textblob_tokenizer,\n",
    "                      stop_words='english',\n",
    "                      use_idf=True)\n",
    "matrix = vec.fit_transform(texts)\n",
    "idf_df = pd.DataFrame(matrix.toarray(), columns=vec.get_feature_names())\n",
    "idf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# KMeans clustering a kind of clustering.\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "number_of_clusters = 2\n",
    "km = KMeans(n_clusters=number_of_clusters)\n",
    "km.fit(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vec.get_feature_names()\n",
    "for i in range(number_of_clusters):\n",
    "    top_ten_words = [terms[ind] for ind in order_centroids[i, :5]]\n",
    "    print(\"Cluster {}: {}\".format(i, ' '.join(top_ten_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame()\n",
    "results['text'] = texts\n",
    "results['category'] = km.labels_\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How about 3 categories of documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That was confusing. Can we visualize it?\n",
    "\n",
    "This time we're going to say, **only find two important words to measure**. We're going to use `max_features=` to have it auto-select, but we could also use `vocabulary=` if we wanted to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = [\n",
    "    'Penny bought bright blue fishes.',\n",
    "    'Penny bought bright blue and orange bowl.',\n",
    "    'The cat ate a fish at the store.',\n",
    "    'Penny went to the store. Penny ate a bug. Penny saw a fish.',\n",
    "    'It meowed once at the bug, it is still meowing at the bug and the fish',\n",
    "    'The cat is at the fish store. The cat is orange. The cat is meowing at the fish.',\n",
    "    'Penny is a fish.',\n",
    "    'Penny Penny she loves fishes Penny Penny is no cat.',\n",
    "    'The store is closed now.',\n",
    "    'How old is that tree?',\n",
    "    'I do not eat fish I do not eat cats I only eat bugs.'\n",
    "]\n",
    "\n",
    "vec = TfidfVectorizer(tokenizer=textblob_tokenizer,\n",
    "                      stop_words='english',\n",
    "                      use_idf=True,\n",
    "                      max_features=2)\n",
    "matrix = vec.fit_transform(texts)\n",
    "df = pd.DataFrame(matrix.toarray(), columns=vec.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we now have two numbers for every sentence? Well, let's plot them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ax = df.plot(kind='scatter', x='fish', y='penni', alpha=0.2, s=200)\n",
    "ax.set_xlabel(\"Fish\")\n",
    "ax.set_ylabel(\"Penny\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see a few groups. 3 or 4, maybe? Let's see if we can do the same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "number_of_clusters = 3\n",
    "km = KMeans(n_clusters=number_of_clusters)\n",
    "km.fit(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Move the labels into a column of our dataframe\n",
    "# the first label matches the first row, second label is second row, etc\n",
    "df['category'] = km.labels_\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Category 0 is red\n",
    "# Category 1 is green\n",
    "# Category 2 is blue\n",
    "colormap = {\n",
    "    0: 'red',\n",
    "    1: 'green',\n",
    "    2: 'blue'\n",
    "}\n",
    "\n",
    "# Create a list of colors from every single row\n",
    "colors = df.apply(lambda row: colormap[row.category], axis=1)\n",
    "\n",
    "# And plot it!\n",
    "ax = df.plot(kind='scatter', x='fish', y='penni', alpha=0.1, s=300, c=colors)\n",
    "ax.set_xlabel(\"Fish\")\n",
    "ax.set_ylabel(\"Penny\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ooh, that's fun, right? Let's try it again, this time with **four categories** instead of three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=4)\n",
    "km.fit(matrix)\n",
    "df['category'] = km.labels_\n",
    "\n",
    "colormap = { 0: 'red', 1: 'green', 2: 'blue', 3: 'purple'}\n",
    "colors = df.apply(lambda row: colormap[row.category], axis=1)\n",
    "ax = df.plot(kind='scatter', x='fish', y='penni', alpha=0.1, s=300, c=colors)\n",
    "ax.set_xlabel(\"Fish\")\n",
    "ax.set_ylabel(\"Penny\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now just imagine instead of 2 dimensions (2 words), you have 100 dimensions (100 words). It's more complicated and you sure can't visualize it, but it's the same thing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using more information\n",
    "\n",
    "Right now we're only vectorizing **Penny** and **fish** - remember how we did `max_features`? Right now it's only doing term frequency across those two elements - it doesn't matter if there are 10000 words in a book, if \"Penny\" shows up once and \"fish\" shows up twice, the vectorizer is like \"OH BOY THIS IS ALL ABOUT FISH.\"\n",
    "\n",
    "If we wanted it to be a little more aware of the rest of the words, we could do our vectorization across *all* features (all words), then only selecting the `fish` and `penni` columns when *doing K-means fit*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vectorize and save into a new dataframe\n",
    "vec = TfidfVectorizer(tokenizer=textblob_tokenizer, stop_words='english', use_idf=True)\n",
    "matrix = vec.fit_transform(texts)\n",
    "df = pd.DataFrame(matrix.toarray(), columns=vec.get_feature_names())\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now that we have a count of ALL of the words, let's ask K-Means to only pay attention to `fish` and `penni`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cluster with 3 categories\n",
    "# only using the 'fish' and 'penni' categories\n",
    "km = KMeans(n_clusters=3)\n",
    "km.fit(df[['fish', 'penni']])\n",
    "\n",
    "# Assign the category to the dataframe\n",
    "df['category'] = km.labels_\n",
    "\n",
    "# Build our color map\n",
    "colormap = { 0: 'red', 1: 'green', 2: 'blue' }\n",
    "colors = df.apply(lambda row: colormap[row.category], axis=1)\n",
    "\n",
    "# Plot our scatter\n",
    "ax = df.plot(kind='scatter', x='fish', y='penni', alpha=0.1, s=300, c=colors)\n",
    "ax.set_xlabel(\"Fish\")\n",
    "ax.set_ylabel(\"Penny\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we normally do `km.fit(matrix)` but this time we did `km.fit(df[['fish', 'penni']])`? It turns out that **you can use `matrix` and `df` interchangeably**. The `df` is just the matrix with column names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to get crazy\n",
    "\n",
    "What if we're talking about **3 features?** 3 different words? It doesn't seem that nuts, but... can we graph that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vectorize and save into a new dataframe\n",
    "vec = TfidfVectorizer(tokenizer=textblob_tokenizer, max_features=3, stop_words='english', use_idf=True)\n",
    "matrix = vec.fit_transform(texts)\n",
    "df = pd.DataFrame(matrix.toarray(), columns=vec.get_feature_names())\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cluster\n",
    "km = KMeans(n_clusters=4)\n",
    "km.fit(df)\n",
    "\n",
    "# Assign the category to the dataframe\n",
    "df['category'] = km.labels_\n",
    "\n",
    "# Build our color map\n",
    "colormap = {0: 'red', 1: 'green', 2: 'blue', 3: 'orange'}\n",
    "colors = df.apply(lambda row: colormap[row.category], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot our scatter\n",
    "ax = df.plot(kind='scatter', x='fish', y='penni', alpha=0.2, s=300, c=colors)\n",
    "ax.set_xlabel(\"Fish\")\n",
    "ax.set_ylabel(\"Penny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot our scatter\n",
    "ax = df.plot(kind='scatter', x='penni', y='cat', alpha=0.2, s=300, c=colors)\n",
    "ax.set_xlabel(\"Penni\")\n",
    "ax.set_ylabel(\"Cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def draw(ax, df):\n",
    "    colormap = { 0: 'red', 1: 'green', 2: 'blue', 3: 'orange' }\n",
    "    colors = df.apply(lambda row: colormap[row.category], axis=1)\n",
    "\n",
    "    ax.scatter(df['fish'], df['penni'], df['cat'], c=colors, s=100, alpha=0.5)\n",
    "    ax.set_xlabel('Fish')\n",
    "    ax.set_ylabel('Penni')\n",
    "    ax.set_zlabel('Cat')\n",
    "\n",
    "chart_count_vert = 5\n",
    "chart_count_horiz = 5\n",
    "number_of_graphs = chart_count_vert * chart_count_horiz\n",
    "\n",
    "fig = plt.figure(figsize=(3 * chart_count_horiz, 3 * chart_count_vert))\n",
    "\n",
    "for i in range(number_of_graphs):\n",
    "    ax = fig.add_subplot(chart_count_horiz, chart_count_vert, i + 1, projection='3d', azim=(-360 / number_of_graphs) * i)\n",
    "    draw(ax, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
